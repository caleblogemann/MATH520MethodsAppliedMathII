\documentclass[11pt, oneside]{article}
\usepackage[letterpaper, margin=2cm]{geometry}
\usepackage{MATH520}

\begin{document}
\noindent \textbf{\Large{Caleb Logemann \\
MATH 520 Methods of Applied Math II \\
Homework 11
}}

\subsection*{Section 16.8}
\begin{enumerate}
  \item[\#6] % Done
    Prove the version of the Poincar\'e inequality stated in Proposition 16.1.
    (Suggestions: If no such $C$ exists show that we can find sequence
    $u_k \in H^1_*(\Omega)$ with $\norm[L^2(\Omega)]{u_k} = 1$ such that
    $\norm[L^2(\Omega)]{\nabla u_k} < \frac{1}{k}$.
    Using Rellich's theorem obtain a convergent subsequence whose limit must
    have contradictory properties.

    \begin{proof}
      Suppose to the contrary that no such $C$ exists, such that
      \[
        \norm[L^2(\Omega)]{u} < C \norm[L^2(\Omega)]{\nabla u} \qquad \forall u \in H^1_*(\Omega).
      \]
      This implies that for every $k \in \NN$ there exists some
      $v_k \in H^1_*(\Omega)$ such that
      \[
        \norm[L^2(\Omega)]{v_k} \ge k \norm[L^2(\Omega)]{\nabla v_k}
      \]
      Note that this inequality still holds for
      $u_k = v_k/\norm[L^2(\Omega)]{v_k}$ as
      \begin{align*}
        \norm[L^2(\Omega)]{u_k} &\ge k \norm[L^2(\Omega)]{\nabla u_k} \\
        \norm[L^2(\Omega)]{v_k/\norm[L^2(\Omega)]{v_k}} &\ge k \norm[L^2(\Omega)]{\nabla v_k/\norm[L^2(\Omega)]{v_k}} \\
        \frac{1}{\norm[L^2(\Omega)]{v_k}} \norm[L^2(\Omega)]{v_k} &\ge \frac{1}{\norm[L^2(\Omega)]{v_k}} k \norm[L^2(\Omega)]{\nabla v_k} \\
        \norm[L^2(\Omega)]{v_k} &\ge k \norm[L^2(\Omega)]{\nabla v_k}.
      \end{align*}
      This now shows that
      \[
        \norm[L^2(\Omega)]{u_k} = 1
      \]
      and that
      \[
        \norm[L^2(\Omega)]{\nabla u_k} \le \frac{1}{k}.
      \]

      We can now compute the $H^1(\Omega)$ norm of $u_k$ to be
      \begin{align*}
        \norm[H^1(\Omega)]{u} &= \sqrt{\norm[L^2(\Omega)]{u_k}^2 + \norm[L^2(\Omega)]{\nabla u_k}^2} \\
        &\le \sqrt{1 + \frac{1}{k^2}} \\
        &\le \sqrt{2}
      \end{align*}
      This shows that the set $\set{u_k}$ is bounded in $H^1(\Omega)$.
      Since $\set{u_k}$ is bounded in $H^1(\Omega)$, there exists a weakly
      convergent subsequence $\set{u_{k_j}}$.

      Now assuming that $\Omega$ has smooth enough boundaries, the
      Rellich-Kondrachov theorem as well as the compact embedding of
      $H^1(\Omega)$ in $L^2(\Omega)$ implies that $\set{u_{k_j}}$ is strongly
      convergent in $L^2(\Omega)$.
      Let $u \in L^2(\Omega)$ be the limit of the subsequence $\set{u_{k_j}}$, that is
      $u_{k_j} \to u$ in $L^2(\Omega)$.
      This also implies that $\norm[L^2(\Omega)]{u} = 1$ as
      $\norm[L^2(\Omega)]{u_{k_j}} = 1$.
      Note that strong convergence also implies that $u_{k_j} \wto u$ in
      $L^2(\Omega)$.
      Therefore
      \[
        \abr[L^2(\Omega)]{u_{k_j}, 1} \to \abr[L^2(\Omega)]{u, 1} = \dintt{\Omega}{}{u}{x}
      \]
      but note that since $u_{k_j} \in H^1_*(\Omega)$ as well
      \[
        \abr[L^2(\Omega)]{u_{k_j}, 1} = \dintt{\Omega}{}{u_{k_j}}{x} = 0.
      \]
      This shows that $\dintt{\Omega}{}{u}{x} = 0$.

      Since $\set{u_{k_j}} \wto u$ in $H^1(\Omega)$, Proposition 13.1 states
      that
      \[
        \norm[H^1(\Omega)]{u}^2 \le \liminf[j \to \infty]{\norm[H^1(\Omega)]{u_{k_j}}^2}
      \]
      This can be simplified as follows.
      \begin{align*}
        \norm[H^1(\Omega)]{u}^2 &\le \liminf[j \to \infty]{\norm[H^1(\Omega)]{u_{k_j}}^2} \\
        \norm[L^2(\Omega)]{u}^2 + \norm[L^2(\Omega)]{\nabla u}^2 &\le \liminf[j \to \infty]{\norm[L^2(\Omega)]{u_{k_j}}^2 + \norm[L^2(\Omega)]{\nabla u_{k_j}}^2} \\
        1 + \norm[L^2(\Omega)]{\nabla u}^2 &\le \liminf[j \to \infty]{\norm[L^2(\Omega)]{\nabla u_{k_j}}^2} + 1 \\
        \norm[L^2(\Omega)]{\nabla u}^2 &\le \liminf[j \to \infty]{\norm[L^2(\Omega)]{\nabla u_{k_j}}^2} \\
        \norm[L^2(\Omega)]{\nabla u}^2 &\le 0 \\
      \end{align*}
      This shows that $\nabla u = 0$.
      If a function $u$ has a zero gradient, this implies that $u$ is constant.
      However since we have already shown that $\dintt{\Omega}{}{u}{x} = 0$, and
      the only constant function with zero average value is zero itself.
      Thus $u = 0$, however this contradicts the fact that
      $\norm[L^2(\Omega)]{u} = 1$.

      Now that we have found a contradiction, this shows that there does exist
      a constant $C$ such that
      \[
        \norm[L^2(\Omega)]{u} \le C \norm[L^2(\Omega)]{\nabla u} \qquad \forall u \in H^1_*(\Omega)
      \]
    \end{proof}

  \pagebreak
  \item[\#8]
    Consider a Lagrangian of the form $\mcL = \mcL(u, p)$ (i.e. it happens not
    to depend on the space variable x) when $N = 1$.
    Show that if $u$ is a solution of the Euler-Lagrange equation then
    \[
      \mcL(u, u') - u' \pd{\mcL}{p}(u, u') = C
    \]
    for some constant C.
    In this way we are able to achieve a reduction of order from a second order
    ODE to a first order ODE.
    Use this observation to redo the derivation of the solution of the hanging
    chain problem.

    \begin{proof}
      Let $u$ be a solution of the Euler-Lagrange equation, where the Lagrangian
      is of the form $\mcL(u, p)$.
    \end{proof}

  \pagebreak
  \item[\#9] % Done
    Find the function $u(x)$ which minimizes
    \[
      J(u) = \dintt{0}{1}{(u'(x) - u(x))^2}{x}
    \]
    among all functions $u \in H^1(0, 1)$ satisfying $u(0) = 0$, $u(1) = 1$.

    The function $u(x)$ which minimizes $J(u)$ will be the function that
    satisfies the Euler-Lagrange equation where the Lagrangian is
    $L(x, u, p) = (p - u)^2$.
    The Euler-Lagrange equation in this case is
    \[
      -2(u' - u) - 2\da{u' - u}{x} = 0
    \]
    This is a second order differential equation which can be solved as follows.
    \begin{align*}
      0 &= -2(u' - u) - 2\da{u' - u}{x} \\
      0 &= -2(u' - u) - 2\p{u'' - u'} \\
      0 &= -2u' + 2u - 2u'' + 2u' \\
      0 &= 2u - 2u'' \\
      2u'' &= 2u \\
      u'' &= u \\
    \end{align*}
    The solutions to this differential equation will be of the form
    \[
      u(x) = c_1 e^x + c_2 e^{-x}
    \]
    The constants $c_1$ and $c_2$ can be found by using the boundary conditions
    $u(0) = 0$ and $u(1) = 1$.
    \begin{align*}
      u(0) = c_1 + c_2 = 0 \\
      c_1 = -c_2 \\
      u(1) = c_1 e + c_2 e^{-1} = 1\\
      1 = -c_2 e + c_2 e^{-1} \\
      1 = c_2(-e + e^{-1}) \\
      c_2 = \frac{1}{e^{-1} - e} \\
      c_2 = \frac{e}{1 - e^2} \\
      c_1 = \frac{e}{e^2 - 1}
    \end{align*}
    Thus the solution to this minimization problem that satisfies $u(0) = 0$ and
    $u(1) = 1$ is
    \[
      u(x) = \frac{e}{e^2 - 1} e^x + \frac{e}{1 - e^2} e^{-x}
    \]

  \pagebreak
  \item[\#10]
    The area of a surface obtained by revolving the graph of $y = u(x)$,
    $0 < x < 1$, about the $x$ axis, is
    \[
      J(u) = 2\pi \dintt{0}{1}{u(x) \sqrt{1 + u'(x)^2}}{x}
    \]
    Assume that $u$ is required to satisfy $u(0) = a$, $u(1) = b$ where
    $0 < a < b$.
    \begin{enumerate}
      \item[(a)] % Done
        Find the Euler-Lagrange equation for the problem of minimizing this surface area.

        In this case the Lagrangian is
        \[
          \mcL(x, u, p) = 2\pi u \sqrt{1 + p^2}
        \]
        The Euler-Lagrange equation is then
        \[
          \pd{\mcL}{u} - \pd{}{x} \pd{\mcL}{p} = 0
        \]
        or more specifically
        \[
          2\pi \sqrt{1 + u'} - 2\pi \pd{}{x} \frac{uu'}{\sqrt{1 + u'}} = 0.
        \]
        Simplifying this gives a the final Euler-Lagrange equation to be
        \[
          \sqrt{1 + u'} - \pd{}{x} \frac{uu'}{\sqrt{1 + u'}} = 0.
        \]

      \item[(b)] % Done
        Show that
        \[
          \frac{u(u')^2}{\sqrt{1 + (u')^2}} - u\sqrt{1 + (u')^2}
        \]
        is a constant function for any such minimal surface (Hint: use Exercise 8).

        The Lagrangian for this problem satisifies the assumption made in
        Exercise 8, therefore the following first order differential equation
        must be satisfied for any minimal surface,
        \[
          \mcL(u, u') - u' \pd{\mcL}{p}(u, u') = C.
        \]
        More specifically
        \[
          2\pi u\sqrt{1 + (u')^2} - 2\pi u' \frac{uu'}{\sqrt{1 + (u')^2}} = C.
        \]
        This can be simplified to 
        \[
          \frac{u(u')^2}{\sqrt{1 + (u')^2}} - u\sqrt{1 + (u')^2} = -\frac{C}{2\pi}
        \]

        This shows that
        \[
          \frac{u(u')^2}{\sqrt{1 + (u')^2}} - u\sqrt{1 + (u')^2}
        \]
        is a constant function for any minimal surface.

      \item[(c)]
        Solve the first order ODE in part b) to find the minimal surface.
        Make sure to compute all constants of integration.

        The first order ODE in (b) can be solved as follows.
        \begin{align*}
          \frac{u(u')^2}{\sqrt{1 + (u')^2}} - u\sqrt{1 + (u')^2} &= c \\
          u(u')^2 - u\p{1 + (u')^2} &= c\sqrt{1 + (u')^2} \\
          u\p{(u')^2 - 1 + (u')^2} &= c\sqrt{1 + (u')^2} \\
          -u &= c\sqrt{1 + (u')^2} \\
          u^2 &= c^2 + c^2(u')^2 \\
          u^2 - c^2 &= c^2(u')^2 \\
          \pm \sqrt{u^2 - c^2} &= cu' \\
          \pm c\sqrt{\p{\frac{u}{c}}^2 - 1} &= cu' \\
          \pm \sqrt{\p{\frac{u}{c}}^2 - 1} &= u' \\
          \pm \sqrt{\p{\frac{u}{c}}^2 - 1} &= \d{u}{x} \\
          \pm \intt{}{x} &= \intt{\frac{1}{\sqrt{\p{\frac{u}{c}}^2 - 1}}}{u} \\
          x + d &= c \arccosh{\frac{u}{c}} \\
          u(x) &= c \cosh{\frac{x + d}{c}}
        \end{align*}
        Note that this is equivalent to the solution found earlier for the
        hanging chain, and this is consistent as the Lagrangian is equivalent
        in these situations.
    \end{enumerate}

  \pagebreak
  \item[\#18] % Done
    Show that if $\Omega$ is a bounded domain in $\RR^N$ and $f \in L^2(\Omega)$,
    then the problem of minimizing
    \[
      J(u) = \frac{1}{2} \dintt{\Omega}{}{\abs{\nabla u}^2}{x} - \dintt{\Omega}{}{f u}{x}
    \]
    over $H^1_0(\Omega)$ satisfies all of the conditions of Theorem 16.8.
    What goes wrong if we replace $H^1_0(\Omega)$ by $H^1(\Omega)$.

    \begin{proof}
      First note that $H^1_0(\Omega)$ is a closed subspace of a Hilbert space,
      therefore by Theorem 13.1 $H^1_0(\Omega)$ is weakly closed.

      Now I will show that $J(u)$ is both coersive and weakly lower
      semicontinuous.
      To see that $J(u)$ is coersive consider a sequence
      $u_k \in H^1_0(\Omega)$, such that $\norm[H^1_0(\Omega)]{u_k} \to \infty$.
      Consider the following, which follows from the Cauchy-Schwartz are
      Poincar\'e inequalities.
      \[
        \dintt{\Omega}{}{fu}{x} &\le \norm[L^2]{f} \norm[L^2]{u} &\le C\norm[L^2]{f} \norm[H^1_0]{u}
      \]
      Using this inequality we can characterize $J(u_k)$ as
      \begin{align*}
        J(u_k) &= \frac{1}{2} \dintt{\Omega}{}{\abr{\nabla u_k}^2}{x} - \dintt{\Omega}{}{f u_k}{c} \\
        &\ge \frac{1}{2} \norm[H^1_0(\Omega)]{u_k}^2 - C \norm[L^2]{f} \norm[H^1_0]{u_k} \\
        &= \norm[H^1_0(\Omega)]{u_k} \p{\frac{1}{2} \norm[H^1_0(\Omega)]{u_k} - C \norm[L^2]{f}} \\
      \end{align*}
      Since $C \norm[L^2]{f}$ is fixed we can see that this last expression
      tends to infinity as $\norm[H^1_0(\Omega)]{u_k} \to \infty$ and
      $\frac{1}{2} \norm[H^1_0(\Omega)]{u_k} - C \norm[L^2]{f} \to \infty$.
      Therefore $J(u_k) \to \infty$, and thus $J$ is coersive.
      Note that this proof wouldn't work for $u_k \in H^1(\Omega)$ as
      $\norm[L^2(\Omega)]{u_k} \to \infty$ does not imply that
      $\norm[L^2(\Omega)]{\nabla u_k} \to \infty$.

      Lastly I will show that $J$ is weakly lower semicontinuous.
      In order to do this suppose that $u_k \wto u$ in $H^1_0(\Omega)$.
      Since $H^1_0(\Omega)$ is compact in $L^2(\Omega)$ this implies that
      $u_k \to u$ in $L^2(\Omega)$
      Since $u_k \to u$ in $L^2(\Omega)$, it is also known that $u_k \wto u$ in
      $L^2(\Omega)$ and this implies that
      $\dintt{\Omega}{}{f u_k}{x} \to \dintt{\Omega}{}{f u}{x}$.
      Proposition 13.2 also gives that
      \[
        \norm[H^1_0(\Omega)]{u}^2 \le \liminf[k \to \infty]{\norm[H^1_0(\Omega)]{u_k}^2}
      \]
      or equivalently that
      \[
        \dintt{\Omega}{}{\abs{\nabla u}^2}{x} \le \liminf[k \to \infty]{\dintt{\Omega}{}{\abs{\nabla u_k}^2}{x}}.
      \]
      Now using these two facts together we see that
      \begin{align*}
        J(u) &= \frac{1}{2}\dintt{\Omega}{}{\abs{\nabla u}^2}{x} - \dintt{\Omega}{}{fu}{x} \\
        &\le \liminf[k \to \infty]{\frac{1}{2}\dintt{\Omega}{}{\abs{\nabla u_k}^2}{x} - \dintt{\Omega}{}{fu_k}{x}} \\
        &= \liminf[k \to \infty]{J(u_k)}
      \end{align*}
      Thus $J$ is weakly lower semicontinuous and satisfies all of the
      assumptions of Theorem 16.8.
    \end{proof}

  \pagebreak
  \item[\#19] % Done
    We say that $J: \mcX \to \RR$ is stricly convex if
    \[
      J(tx + (1 - t)y) < tJ(x) + (1 - t)J(y) \qquad x, y \in \mcX \quad 0 < t < 1
    \]
    If $J$ is strictly convex show that the minimization problem (16.6.89) has
    at most one solution.

    \begin{proof}
      First note that in order for $J$ to be strictly convex $\mcX$ must be a
      convex set as well.
      This means that if $x, y \in \mcX$, then $tx + (1 - t)y \in \mcX$ for all
      $t \in \br{0, 1}$.
      Suppose that there exists two distinct solutions to the minimization
      problem (16.6.89), that is there exists $x, y \in \mcX$ such that
      $x \neq y$ and $J(x) = J(y) = \min*[z \in \mcX]{J(z)}$.
      Now since $J$ is strictly convex it is known that $x/2 + y/2 \in \mcX$ and
      \[
        J(x/2 + y/2) < J(x)/2 + J(y)/2 = J(x)
      \]
      This is a contradiction as $J(x) = \min*[z \in \mcX]{J(x)}$.
      Therefore $J$ has a most one solution to the minimization problem.
    \end{proof}
\end{enumerate}
\end{document}
